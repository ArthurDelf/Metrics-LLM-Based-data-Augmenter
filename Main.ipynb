{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fac6bd95-e2db-4696-83e2-b15711b09b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Assure que le GPU est bien utilis√©\n",
    "print(\"f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b153bb7-f01c-42c2-a23a-d8894d1a099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed0bb46e-91ba-4bc5-a731-424b3b26f2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 563,200 || all params: 1,100,611,584 || trainable%: 0.0512\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Nom du mod√®le\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Configurer la quantization en 4-bit\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# Charger le mod√®le avec quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Configuration de LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=4,  # R√©duction de rang pour √©conomiser la m√©moire\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Appliquer LoRA sur les couches d'attention\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Appliquer LoRA au mod√®le\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff929b78-cecb-4545-aa23-dc71bd0479b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger un dataset depuis un fichier JSON local\n",
    "dataset = load_dataset(\"json\", data_files=r\"C:\\Users\\arthu\\Downloads\\data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e802dc0-1745-45d2-a4ca-1b92d6cb8447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': 'How to use the seq() function to generate values for an empty vector in R?', 'question': \"I'm trying to fill an empty vector with values in R using the seq() function. I want to generate values from 1 to 10 with a step of 2. How can I do this?\", 'answer': \"You can use the seq() function to generate values for an empty vector in R as follows: x <- seq(1, 10, by = 2). The 'by' argument specifies the step size. This will generate the following vector: [1] 1 3 5 7 9. If you want to include 10 in the sequence, you can use the 'length.out' argument: x <- seq(1, 10, by = 2, length.out = 6).\", 'text': \"Contexte: How to use the seq() function to generate values for an empty vector in R?\\n\\nQuestion: I'm trying to fill an empty vector with values in R using the seq() function. I want to generate values from 1 to 10 with a step of 2. How can I do this?\\n\\nR√©ponse: You can use the seq() function to generate values for an empty vector in R as follows: x <- seq(1, 10, by = 2). The 'by' argument specifies the step size. This will generate the following vector: [1] 1 3 5 7 9. If you want to include 10 in the sequence, you can use the 'length.out' argument: x <- seq(1, 10, by = 2, length.out = 6).\"}\n"
     ]
    }
   ],
   "source": [
    "def format_example(example):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "    return {\n",
    "        \"text\": f\"Contexte: {context}\\n\\nQuestion: {question}\\n\\nR√©ponse: {answer}\"\n",
    "    }\n",
    "\n",
    "# Appliquer la transformation\n",
    "dataset = dataset.map(format_example)\n",
    "\n",
    "# Afficher un exemple reformatt√©\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e504ac9-7f58-4561-b323-87abdb2d24e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# V√©rifie si CUDA est dispo\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# D√©place le mod√®le sur le GPU si possible\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# V√©rifie si CUDA est dispo\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# D√©place le mod√®le sur le GPU si possible\n",
    "model.to(device)\n",
    "\n",
    "# Configurer le collateur de donn√©es (G√®re le padding automatiquement)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, padding=True)\n",
    "\n",
    "# Fonction pour transformer le dataset en un format utilisable par le mod√®le\n",
    "def preprocess_function(examples):\n",
    "    text = f\" Contexte:\\n{examples['context']}\\n\\n Question:\\n{examples['question']}\\n\\n R√©ponse:\\n{examples['answer']}\"\n",
    "\n",
    "    # Tokenisation du texte\n",
    "    tokenized = tokenizer(text, truncation=True, padding=\"max_length\", max_length=2048)\n",
    "\n",
    "    # Les labels doivent √™tre une copie des input_ids avec -100 pour les tokens √† ignorer\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "# Appliquer le preprocessing au dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# üìå **ICI que tu dois mettre training_args**\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinyllama_lora\",\n",
    "    per_device_train_batch_size=2,  # Ajuste selon la m√©moire dispo\n",
    "    gradient_accumulation_steps=4,  # Simule un batch plus grand\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,  # Active le float16 pour √©conomiser la VRAM\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# Initialisation de Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset[\"train\"].shuffle().select(range(2000)),  # R√©duit la taille pour √©viter l'overflow m√©moire\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Lancer l'entra√Ænement\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c186ace9-8a52-47a5-960e-07bcde5f7dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde de l'adaptateur LoRA\n",
    "model.save_pretrained(\"tinyllama_lora_adapter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21905259-5a9f-40fa-a642-79a09c8cbf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "554bc940-383c-4c74-ae36-c73c6118c978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le charg√© avec succ√®s !\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Optimisation pour GPU\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Format optimis√©\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,  # Remplace `load_in_4bit`\n",
    "    device_map=\"auto\"  # Utilise automatiquement le GPU\n",
    ")\n",
    "\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, \"tinyllama_lora_adapter\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Mod√®le charg√© avec succ√®s !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4130529-a16b-4c2d-ac0c-b1d9e1007be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\n",
      "How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\n",
      "\n",
      "A: You can use the following code to generate different column selections based on multiple vectors and output the results in an R console:\n",
      "\n",
      "```r\n",
      "# Create a data frame with three columns\n",
      "df <- data.frame(a = c(1, 2, 3), b = c(4, 5, 6), c = c(7, 8, 9))\n",
      "\n",
      "# Create a vector of column names\n",
      "colnames <- c('a', 'b', 'c')\n",
      "\n",
      "# Create a vector of column indices\n",
      "indices <- c(1, 2, 3)\n",
      "\n",
      "# Create a loop to generate column selections\n",
      "for (i in 1:length(indices)) {\n",
      "  # Create a new column based on the selected column indices\n",
      "  df <- df\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=200)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e23e9fd5-2538-4df1-81bc-e93b843b9c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c07ef4f-22ef-44ab-ac73-42a98cea0f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\arthu\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--rouge\\b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886 (last modified on Fri Mar 14 11:56:17 2025) since it couldn't be found locally at evaluate-metric--rouge, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_tokens=200):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = model.generate(**inputs, max_new_tokens=max_tokens)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# ---- M√âTRIQUES ---- #\n",
    "\n",
    "def compute_perplexity(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    \n",
    "    loss = outputs.loss\n",
    "    return math.exp(loss.item())\n",
    "\n",
    "def compute_bleu(reference, generated):\n",
    "    return sentence_bleu([reference.split()], generated.split())\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_rouge(reference, generated):\n",
    "    return rouge.compute(predictions=[generated], references=[reference])\n",
    "\n",
    "def compute_meteor(reference, generated):\n",
    "    return meteor_score([reference.split()], generated.split())\n",
    "\n",
    "def distinct_ngrams(text, n=1):\n",
    "    words = text.split()\n",
    "    ngrams = set(zip(*[words[i:] for i in range(n)]))\n",
    "    return len(ngrams) / len(words) if words else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45359bf6-9241-4123-9594-c14065018b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\"\n",
    "generated_text = generate_text(model, tokenizer, prompt)\n",
    "reference_text = \"Deep learning is a subfield of machine learning that uses neural networks to learn from data.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa14601b-2add-4294-b82b-dd9bf0722914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\n",
      "How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\n",
      "\n",
      "A: You can use the following code to generate different column selections based on multiple vectors and output the results in an R console:\n",
      "\n",
      "```r\n",
      "# Create a data frame with three columns\n",
      "df <- data.frame(a = c(1, 2, 3), b = c(4, 5, 6), c = c(7, 8, 9))\n",
      "\n",
      "# Create a vector of column names\n",
      "colnames <- c('a', 'b', 'c')\n",
      "\n",
      "# Create a vector of column indices\n",
      "indices <- c(1, 2, 3)\n",
      "\n",
      "# Create a loop to generate column selections\n",
      "for (i in 1:length(indices)) {\n",
      "  # Create a new column based on the selected column indices\n",
      "  df <- df\n",
      "\n",
      "Perplexity: 1.8280167003839216\n",
      "ROUGE Scores: {'rouge1': 0.050314465408805034, 'rouge2': 0.0, 'rougeL': 0.03773584905660377, 'rougeLsum': 0.050314465408805034}\n",
      "Dist-1: 0.4489795918367347, Dist-2: 0.6122448979591837\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Generated Text: {generated_text}\\n\")\n",
    "print(f\"Perplexity: {compute_perplexity(model, tokenizer, generated_text)}\")\n",
    "#print(f\"BLEU Score: {compute_bleu(reference_text, generated_text)}\")\n",
    "print(f\"ROUGE Scores: {compute_rouge(reference_text, generated_text)}\")\n",
    "#print(f\"METEOR Score: {compute_meteor(reference_text, generated_text)}\")\n",
    "print(f\"Dist-1: {distinct_ngrams(generated_text, 1)}, Dist-2: {distinct_ngrams(generated_text, 2)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8741e2bc-dda1-435b-b3ce-a40811bb3a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecb4fa8af9246a1b03dae03b24a6841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthu\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\arthu\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355399f924bc46108e447c34a1c2b677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58525354d5714c9087ad6dcbe83ab9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8dd4747bde14943b9dc960a285eabd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5488e88fda9f41f79e6141cc795b96f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d6d358840f458e8337a08a439cded9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83fb1945f4624bf295037b97da7d610d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7a74dd4-3fef-4927-bf42-afca274e8a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a958d9cc984b5eb7a93e38eabfe5f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/240 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthu\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\arthu\\.cache\\huggingface\\hub\\models--Salesforce--codegen-350M-mono. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ab979b29aa49df91c8109df92719a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ad76e64789418fa4d5a96d4759316e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ce0f7c4da64e61a4ae999246cf23d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62776afcdae489194fbe976e249bc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa94227ff1343b587d8c6717f109c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560a16d59b3145e2a67b3d42bf93fe69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/999 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b3780b03a24d0ca6d054e425d06f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/797M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57bb0a3c6623461984ff638afb406214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/797M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "codegen_tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
    "codegen_model = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-350M-mono\").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e0917e-0aff-4c5d-94c8-83aceae033b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Charger le fichier data.json\n",
    "with open(r\"C:\\Users\\arthu\\Downloads\\data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extraire les prompts et les r√©ponses de r√©f√©rence\n",
    "prompts = [entry[\"question\"] for entry in data]\n",
    "references = [entry[\"answer\"] for entry in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfc4a6d0-1076-4b0f-8ffb-6de7b802e034",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Initialisation des mod√®les\u001b[39;00m\n\u001b[0;32m      4\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: (tokenizer, model_name),\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m: (gpt2_tokenizer, gpt2_model),\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodegen\u001b[39m\u001b[38;5;124m\"\u001b[39m: (codegen_tokenizer, codegen_model)\n\u001b[0;32m      8\u001b[0m }\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# G√©n√©rer les r√©ponses\u001b[39;00m\n\u001b[0;32m     11\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Charger le fichier data.json ---\n",
    "with open(r\"C:\\Users\\arthu\\Downloads\\data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "prompts = [entry[\"question\"] for entry in data]\n",
    "references = [entry[\"answer\"] for entry in data]\n",
    "\n",
    "# --- G√©n√©rer les r√©ponses avec ton mod√®le TinyLlama ---\n",
    "def generate_responses(model, tokenizer, prompts):\n",
    "    generations = []\n",
    "    model.eval()\n",
    "    for prompt in tqdm(prompts, desc=\"üîÅ G√©n√©ration\"):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, max_new_tokens=300)\n",
    "        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        generations.append(decoded)\n",
    "    return generations\n",
    "\n",
    "tinyllama_outputs = generate_responses(model, tokenizer, prompts)\n",
    "\n",
    "# --- Calcul des m√©triques ---\n",
    "rouge = load(\"rouge\")\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "# ROUGE\n",
    "rouge_result = rouge.compute(predictions=tinyllama_outputs, references=references)\n",
    "print(\"\\nüìä ROUGE:\", rouge_result)\n",
    "\n",
    "# BERTScore\n",
    "bert_result = bertscore.compute(predictions=tinyllama_outputs, references=references, lang=\"en\")\n",
    "print(\"\\nüìä BERTScore (F1 moyenne):\", sum(bert_result[\"f1\"]) / len(bert_result[\"f1\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bec3fab2-46f0-469d-ba88-bc392f608a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è G√©n√©ration avec : TinyLlama+LoRA\n",
      "\n",
      "üß† R√©ponse g√©n√©r√©e par TinyLlama+LoRA :\n",
      "How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\n",
      "How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\n",
      "\n",
      "A: You can use the following code to generate different column selections based on multiple vectors and output the results in an R console:\n",
      "\n",
      "```r\n",
      "# Create a data frame with three columns\n",
      "df <- data.frame(a = c(1, 2, 3), b = c(4, 5, 6), c = c(7, 8, 9))\n",
      "\n",
      "# Create a vector of column names\n",
      "colnames <- c('a', 'b', 'c')\n",
      "\n",
      "# Create a vector of column indices\n",
      "indices <- c(1, 2, 3)\n",
      "\n",
      "# Create a loop to generate column selections\n",
      "for (i in 1:length(indices)) {\n",
      "  # Create a new column based on the selected column indices\n",
      "  df <- df[indices[i], ]\n",
      "  \n",
      "  # Print the results\n",
      "  print(df)\n",
      "}\n",
      "```\n",
      "\n",
      "This code will generate three column selections based on the selected column indices. The loop will iterate over the selected column indices and create a new column based on them. The results will be printed to the console.\n",
      "\n",
      "Note: This code assumes that the data frame has three columns. If the data frame has more columns, you can modify the loop to generate the\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è G√©n√©ration avec : GPT-2\n",
      "\n",
      "üß† R√©ponse g√©n√©r√©e par GPT-2 :\n",
      "How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\n",
      "\n",
      "You can use a loop to generate different column selections based on multiple vectors and output the results in an R console.\n",
      "\n",
      "You can use a loop to generate different column selections based on multiple vectors and output the results in an R console.\n",
      "\n",
      "You can use a loop to generate different column selections based on multiple vectors and output the results in an R console.\n",
      "\n",
      "You can use a loop to generate different column selections based on multiple vectors and output the results in an R console.\n",
      "\n",
      "You can use a loop to generate different column selections based on multiple vectors and output the results in an R console.\n",
      "\n",
      "You can use a loop to generate different column selections based on multiple vectors and output the results in an R console.\n",
      "\n",
      "You can use a loop to generate different column selections based on multiple vectors and output the results in an R console.\n",
      "\n",
      "You can use a loop to generate different column selections based on multiple vectors and output the results in an R console.\n",
      "\n",
      "You can use a loop to generate different column selections based on multiple vectors and output the results in an R console.\n",
      "\n",
      "You can use a loop to generate different column selections based on multiple vectors and output the results in an R console.\n",
      "\n",
      "You can use a loop to generate different column selections based on multiple vectors and output the results in an R console.\n",
      "\n",
      "You can use a loop to generate different column selections based on multiple vectors and output the results in an R console.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è G√©n√©ration avec : CodeGen-350M\n",
      "\n",
      "üß† R√©ponse g√©n√©r√©e par CodeGen-350M :\n",
      "How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\n",
      "\n",
      "# In[ ]:\n",
      "\n",
      "\n",
      "# Create a list of the values in the x and y axis\n",
      "x = [1, 2, 3, 4, 5]\n",
      "y = [1, 2, 3, 4, 5]\n",
      "\n",
      "# Create a list of the values in the x and y axis\n",
      "x = [1, 2, 3, 4, 5]\n",
      "y = [1, 2, 3, 4, 5]\n",
      "\n",
      "# Create a list of the values in the x and y axis\n",
      "x = [1, 2, 3, 4, 5]\n",
      "y = [1, 2, 3, 4, 5]\n",
      "\n",
      "# Create a list of the values in the x and y axis\n",
      "x = [1, 2, 3, 4, 5]\n",
      "y = [1, 2, 3, 4, 5]\n",
      "\n",
      "# Create a list of the values in the x and y axis\n",
      "x = [1, 2, 3, 4, 5]\n",
      "y = [1, 2, 3, 4, 5]\n",
      "\n",
      "# Create a list of the values in the x and y axis\n",
      "x = [1, 2, 3, 4, 5]\n",
      "y = [1, 2, 3, 4, 5]\n",
      "\n",
      "# Create a list of the values in the x and y axis\n",
      "x = [1, 2, 3, 4, 5]\n",
      "y = [1,\n",
      "\n",
      "\n",
      "üìä R√©sum√© des performances :\n",
      "\n",
      "                ROUGE-1  ROUGE-2  ROUGE-L  BERTScore (F1)  Dist-1  Dist-2\n",
      "TinyLlama+LoRA   0.2317   0.0506   0.1454          0.8235  0.4247  0.6468\n",
      "GPT-2            0.1248   0.0173   0.1127          0.7792  0.0871  0.0944\n",
      "CodeGen-350M     0.1861   0.0241   0.1463          0.7996  0.1848  0.2286\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Charger les r√©ponses de r√©f√©rence\n",
    "with open(r\"C:\\Users\\arthu\\Downloads\\data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "references = [entry[\"answer\"] for entry in data]\n",
    "\n",
    "# Saisir un prompt dans la console\n",
    "user_prompt = (\"How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\")\n",
    "\n",
    "# Fonction de g√©n√©ration\n",
    "def generate_text(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=300)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Fonction pour calculer la diversit√©\n",
    "def distinct_n(text, n):\n",
    "    tokens = text.split()\n",
    "    ngrams = list(zip(*[tokens[i:] for i in range(n)]))\n",
    "    return len(set(ngrams)) / len(ngrams) if len(ngrams) > 0 else 0\n",
    "\n",
    "# Initialiser les m√©triques\n",
    "rouge = load(\"rouge\")\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "# Dictionnaire de mod√®les √† comparer\n",
    "models_info = {}\n",
    "\n",
    "# === Ton mod√®le TinyLlama avec LoRA ===\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "model_lora = PeftModel.from_pretrained(base_model, \"tinyllama_lora_adapter\")\n",
    "models_info[\"TinyLlama+LoRA\"] = (model_lora, tokenizer_llama)\n",
    "\n",
    "# === GPT-2 Baseline ===\n",
    "tokenizer_gpt2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model_gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "models_info[\"GPT-2\"] = (model_gpt2, tokenizer_gpt2)\n",
    "\n",
    "\n",
    "# === 2. CodeGen 350M ===\n",
    "tokenizer_codegen = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
    "model_codegen = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-350M-mono\").to(\"cuda\")\n",
    "models_info[\"CodeGen-350M\"] = (model_codegen, tokenizer_codegen)\n",
    "\n",
    "\n",
    "# === Comparaison ===\n",
    "results = {}\n",
    "\n",
    "for model_name, (model, tokenizer) in models_info.items():\n",
    "    print(f\"\\n‚öôÔ∏è G√©n√©ration avec : {model_name}\")\n",
    "    generation = generate_text(model, tokenizer, user_prompt)\n",
    "    print(f\"\\nüß† R√©ponse g√©n√©r√©e par {model_name} :\\n{generation}\\n\")\n",
    "\n",
    "    # Calcul des scores\n",
    "    rouge_scores = rouge.compute(predictions=[generation] * len(references), references=references)\n",
    "    bert_result = bertscore.compute(predictions=[generation] * len(references), references=references, lang=\"en\")\n",
    "    bert_avg = sum(bert_result[\"f1\"]) / len(bert_result[\"f1\"])\n",
    "\n",
    "    dist1 = distinct_n(generation, 1)\n",
    "    dist2 = distinct_n(generation, 2)\n",
    "\n",
    "    results[model_name] = {\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BERTScore (F1)\": bert_avg,\n",
    "        \"Dist-1\": dist1,\n",
    "        \"Dist-2\": dist2\n",
    "    }\n",
    "\n",
    "# === R√©sum√© final ===\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results).T\n",
    "print(\"\\nüìä R√©sum√© des performances :\\n\")\n",
    "print(df.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d92e99-b5f2-4f1d-825d-1df8e7565860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "lora_adapter_path = \"tinyllama_lora_adapter\"  # <- ton mod√®le fine-tun√©\n",
    "data_path = r\"C:\\Users\\arthu\\Downloads\\data.json\"\n",
    "prompt = \"How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ========== M√âTRIQUES ==========\n",
    "rouge = load(\"rouge\")\n",
    "bleu = load(\"bleu\")\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "def distinct_n(text, n):\n",
    "    tokens = text.split()\n",
    "    ngrams = list(zip(*[tokens[i:] for i in range(n)]))\n",
    "    return len(set(ngrams)) / len(ngrams) if ngrams else 0\n",
    "\n",
    "# ========== CHARGER LES R√âF√âRENCES ==========\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "references = [entry[\"answer\"] for entry in data]\n",
    "\n",
    "# ========== G√âN√âRATION ==========\n",
    "def generate_text(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=300)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# ========== CHARGER LES MOD√àLES ==========\n",
    "models_info = {}\n",
    "\n",
    "# --- CONFIG 4bit pour TinyLlama ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# --- 0. TinyLlama (base) ---\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(model_name)\n",
    "model_base = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "models_info[\"TinyLlama (base)\"] = (model_base, tokenizer_base)\n",
    "\n",
    "# --- 1. TinyLlama + LoRA ---\n",
    "tokenizer_lora = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model_lora = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "model_lora = PeftModel.from_pretrained(base_model_lora, lora_adapter_path)\n",
    "models_info[\"TinyLlama+LoRA\"] = (model_lora, tokenizer_lora)\n",
    "\n",
    "# ========== √âVALUATION ==========\n",
    "results = {}\n",
    "\n",
    "for model_label, (model, tokenizer) in models_info.items():\n",
    "    print(f\"\\nüîÑ G√©n√©ration avec {model_label}\")\n",
    "    generation = generate_text(model, tokenizer, prompt)\n",
    "    print(f\"\\nüß† R√©ponse g√©n√©r√©e par {model_label} :\\n{generation}\\n\")\n",
    "\n",
    "    # √âvaluation\n",
    "    rouge_scores = rouge.compute(predictions=[generation] * len(references), references=references)\n",
    "    bleu_score = bleu.compute(predictions=[generation], references=[[ref] for ref in references])\n",
    "    bert_result = bertscore.compute(predictions=[generation] * len(references), references=references, lang=\"en\")\n",
    "    bert_avg = sum(bert_result[\"f1\"]) / len(bert_result[\"f1\"])\n",
    "\n",
    "    results[model_label] = {\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"BERTScore (F1)\": bert_avg,\n",
    "        \"Dist-1\": distinct_n(generation, 1),\n",
    "        \"Dist-2\": distinct_n(generation, 2)\n",
    "    }\n",
    "\n",
    "# ========== R√âSULTAT FINAL ==========\n",
    "df = pd.DataFrame(results).T\n",
    "print(\"\\nüìä R√©sum√© des performances :\\n\")\n",
    "print(df.round(4))\n",
    "\n",
    "# Optionnel : export CSV\n",
    "# df.to_csv(\"resultats_modeles.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea220130-3e55-4b7d-800d-14f688b022bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ G√©n√©ration avec TinyLlama (base)\n",
      "\n",
      "üß† R√©ponse g√©n√©r√©e :\n",
      "How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from evaluate import load\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "lora_adapter_path = \"tinyllama_lora_adapter\"  # <- Ton LoRA fine-tun√©\n",
    "data_path = r\"C:\\Users\\arthu\\Downloads\\data.json\"\n",
    "prompt = \"How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ========== M√âTRIQUES UTILES ==========\n",
    "bleu = load(\"bleu\")\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "# Exact match\n",
    "def exact_match(pred, ref):\n",
    "    return int(pred.strip().lower() == ref.strip().lower())\n",
    "\n",
    "# BLEU Accuracy (seuil)\n",
    "def bleu_accuracy(preds, refs, threshold=0.3):\n",
    "    correct = 0\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        score = bleu.compute(predictions=[pred], references=[[ref]])[\"bleu\"]\n",
    "        if score >= threshold:\n",
    "            correct += 1\n",
    "    return correct / len(preds)\n",
    "\n",
    "# BERTScore Accuracy (seuil)\n",
    "def bertscore_accuracy(preds, refs, threshold=0.85):\n",
    "    scores = bertscore.compute(predictions=preds, references=refs, lang=\"en\")[\"f1\"]\n",
    "    return sum(s >= threshold for s in scores) / len(scores)\n",
    "\n",
    "# Exact Match Accuracy global\n",
    "def em_accuracy(preds, refs):\n",
    "    return sum([exact_match(p, r) for p, r in zip(preds, refs)]) / len(refs)\n",
    "\n",
    "# ========== CHARGER LES R√âF√âRENCES ==========\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "references = [entry[\"answer\"] for entry in data]\n",
    "\n",
    "# ========== G√âN√âRATION ==========\n",
    "def generate_text(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=300)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# ========== CHARGER LES MOD√àLES ==========\n",
    "models_info = {}\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True  \n",
    ")\n",
    "\n",
    "# --- 0. TinyLlama (base) ---\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(model_name)\n",
    "model_base = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "models_info[\"TinyLlama (base)\"] = (model_base, tokenizer_base)\n",
    "\n",
    "# --- 1. TinyLlama + LoRA ---\n",
    "tokenizer_lora = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model_lora = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "model_lora = PeftModel.from_pretrained(base_model_lora, lora_adapter_path)\n",
    "models_info[\"TinyLlama+LoRA\"] = (model_lora, tokenizer_lora)\n",
    "\n",
    "# ========== √âVALUATION ==========\n",
    "all_results = {}\n",
    "\n",
    "for model_label, (model, tokenizer) in models_info.items():\n",
    "    print(f\"\\nüîÑ G√©n√©ration avec {model_label}\")\n",
    "    generation = generate_text(model, tokenizer, prompt)\n",
    "    print(f\"\\nüß† R√©ponse g√©n√©r√©e :\\n{generation}\\n\")\n",
    "\n",
    "    predictions = [generation] * len(references)\n",
    "\n",
    "    # Calcul des m√©triques d'accuracy\n",
    "    em = em_accuracy(predictions, references)\n",
    "    bleu_acc = bleu_accuracy(predictions, references)\n",
    "    bert_acc = bertscore_accuracy(predictions, references)\n",
    "\n",
    "    all_results[model_label] = {\n",
    "        \"Exact Match Acc\": em,\n",
    "        \"BLEU Acc\": bleu_acc,\n",
    "        \"BERTScore Acc\": bert_acc\n",
    "    }\n",
    "\n",
    "# ========== AFFICHAGE FINAL ==========\n",
    "df = pd.DataFrame(all_results).T\n",
    "print(\"\\nüìä R√©sum√© des ACCURACIES :\\n\")\n",
    "print(df.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7252fb3-deac-4416-aa2b-971a33b23054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ G√©n√©ration avec TinyLlama (base)\n",
      "\n",
      "üß† R√©ponse g√©n√©r√©e :\n",
      "How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\arthu\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ G√©n√©ration avec TinyLlama+LoRA\n",
      "\n",
      "üß† R√©ponse g√©n√©r√©e :\n",
      "How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\n",
      "How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\n",
      "\n",
      "A: You can use the following code to generate different column selections based on multiple vectors and output the results in an R console:\n",
      "\n",
      "```r\n",
      "# Create a data frame with three columns\n",
      "df <- data.frame(a = c(1, 2, 3), b = c(4, 5, 6), c = c(7, 8, 9))\n",
      "\n",
      "# Create a vector of column names\n",
      "colnames <- c('a', 'b', 'c')\n",
      "\n",
      "# Create a vector of column indices\n",
      "indices <- c(1, 2, 3)\n",
      "\n",
      "# Create a loop to generate column selections\n",
      "for (i in 1:length(indices)) {\n",
      "  # Create a new column based on the selected column indices\n",
      "  df <- df[indices[i], ]\n",
      "  \n",
      "  # Print the results\n",
      "  print(df)\n",
      "}\n",
      "```\n",
      "\n",
      "This code will generate three column selections based on the selected column indices. The loop will iterate over the selected column indices and create a new column based on them. The results will be printed to the console.\n",
      "\n",
      "Note: This code assumes that the data frame has three columns. If the data frame has more columns, you can modify the loop to generate the\n",
      "\n",
      "\n",
      "üìä R√©sum√© des performances :\n",
      "\n",
      "                  Exact Match Acc  BLEU Acc  BERTScore Acc  Accuracy  \\\n",
      "TinyLlama (base)              0.0       0.0         0.1569       0.0   \n",
      "TinyLlama+LoRA                0.0       0.0         0.0731       0.0   \n",
      "\n",
      "                  Precision  Recall  F1 Score  \n",
      "TinyLlama (base)        0.0     0.0       0.0  \n",
      "TinyLlama+LoRA          0.0     0.0       0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthu\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from evaluate import load\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "lora_adapter_path = \"tinyllama_lora_adapter\"  # <- Ton LoRA fine-tun√©\n",
    "data_path = r\"C:\\Users\\arthu\\Downloads\\data.json\"\n",
    "prompt = \"How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ========== M√âTRIQUES UTILES ==========\n",
    "bleu = load(\"bleu\")\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "# Exact match\n",
    "def exact_match(pred, ref):\n",
    "    return int(pred.strip().lower() == ref.strip().lower())\n",
    "\n",
    "# BLEU Accuracy (seuil)\n",
    "def bleu_accuracy(preds, refs, threshold=0.3):\n",
    "    correct = 0\n",
    "    binary_preds = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        score = bleu.compute(predictions=[pred], references=[[ref]])[\"bleu\"]\n",
    "        is_correct = score >= threshold\n",
    "        binary_preds.append(int(is_correct))\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "    return correct / len(preds), binary_preds\n",
    "\n",
    "# BERTScore Accuracy (seuil)\n",
    "def bertscore_accuracy(preds, refs, threshold=0.85):\n",
    "    scores = bertscore.compute(predictions=preds, references=refs, lang=\"en\")[\"f1\"]\n",
    "    return sum(s >= threshold for s in scores) / len(scores)\n",
    "\n",
    "# Exact Match Accuracy global\n",
    "def em_accuracy(preds, refs):\n",
    "    return sum([exact_match(p, r) for p, r in zip(preds, refs)]) / len(refs)\n",
    "\n",
    "# ========== CHARGER LES R√âF√âRENCES ==========\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "references = [entry[\"answer\"] for entry in data]\n",
    "\n",
    "# ========== G√âN√âRATION ==========\n",
    "def generate_text(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=300)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# ========== CHARGER LES MOD√àLES ==========\n",
    "models_info = {}\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# --- 0. TinyLlama (base) ---\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(model_name)\n",
    "model_base = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "models_info[\"TinyLlama (base)\"] = (model_base, tokenizer_base)\n",
    "\n",
    "# --- 1. TinyLlama + LoRA ---\n",
    "tokenizer_lora = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model_lora = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "model_lora = PeftModel.from_pretrained(base_model_lora, lora_adapter_path)\n",
    "models_info[\"TinyLlama+LoRA\"] = (model_lora, tokenizer_lora)\n",
    "\n",
    "# ========== √âVALUATION ==========\n",
    "all_results = {}\n",
    "\n",
    "for model_label, (model, tokenizer) in models_info.items():\n",
    "    print(f\"\\nüîÑ G√©n√©ration avec {model_label}\")\n",
    "    generation = generate_text(model, tokenizer, prompt)\n",
    "    print(f\"\\nüß† R√©ponse g√©n√©r√©e :\\n{generation}\\n\")\n",
    "\n",
    "    predictions = [generation] * len(references)\n",
    "\n",
    "    # Calcul des m√©triques\n",
    "    em = em_accuracy(predictions, references)\n",
    "    bleu_acc, binary_preds = bleu_accuracy(predictions, references)\n",
    "    bert_acc = bertscore_accuracy(predictions, references)\n",
    "\n",
    "    # Classification-based scores\n",
    "    y_true = [1] * len(references)  # on suppose que toutes les r√©ponses attendues sont positives\n",
    "    acc = accuracy_score(y_true, binary_preds)\n",
    "    precision = precision_score(y_true, binary_preds)\n",
    "    recall = recall_score(y_true, binary_preds)\n",
    "    f1 = f1_score(y_true, binary_preds)\n",
    "\n",
    "    all_results[model_label] = {\n",
    "        \"Exact Match Acc\": em,\n",
    "        \"BLEU Acc\": bleu_acc,\n",
    "        \"BERTScore Acc\": bert_acc,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1\n",
    "    }\n",
    "\n",
    "# ========== AFFICHAGE FINAL ==========\n",
    "df = pd.DataFrame(all_results).T\n",
    "print(\"\\nüìä R√©sum√© des performances :\\n\")\n",
    "print(df.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f158068-2189-4e6c-8dbd-62ca610f0df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ G√©n√©ration avec TinyLlama (base)\n",
      "\n",
      "üß† R√©ponse g√©n√©r√©e :\n",
      "How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ G√©n√©ration avec TinyLlama+LoRA\n",
      "\n",
      "üß† R√©ponse g√©n√©r√©e :\n",
      "How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\n",
      "How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\n",
      "\n",
      "A: You can use the following code to generate different column selections based on multiple vectors and output the results in an R console:\n",
      "\n",
      "```r\n",
      "# Create a data frame with three columns\n",
      "df <- data.frame(a = c(1, 2, 3), b = c(4, 5, 6), c = c(7, 8, 9))\n",
      "\n",
      "# Create a vector of column names\n",
      "colnames <- c('a', 'b', 'c')\n",
      "\n",
      "# Create a vector of column indices\n",
      "indices <- c(1, 2, 3)\n",
      "\n",
      "# Create a loop to generate column selections\n",
      "for (i in 1:length(indices)) {\n",
      "  # Create a new column based on the selected column indices\n",
      "  df <- df[indices[i], ]\n",
      "  \n",
      "  # Print the results\n",
      "  print(df)\n",
      "}\n",
      "```\n",
      "\n",
      "This code will generate three column selections based on the selected column indices. The loop will iterate over the selected column indices and create a new column based on them. The results will be printed to the console.\n",
      "\n",
      "Note: This code assumes that the data frame has three columns. If the data frame has more columns, you can modify the loop to generate the\n",
      "\n",
      "\n",
      "üìä R√©sum√© des performances :\n",
      "\n",
      "                  BERTScore Acc  Accuracy  Precision  Recall  F1 Score\n",
      "TinyLlama (base)         0.1569    0.1569        1.0  0.1569    0.2712\n",
      "TinyLlama+LoRA           0.0731    0.0731        1.0  0.0731    0.1363\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from evaluate import load\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "lora_adapter_path = \"tinyllama_lora_adapter\"  # <- Ton LoRA fine-tun√©\n",
    "data_path = r\"C:\\Users\\arthu\\Downloads\\data.json\"\n",
    "prompt = \"How can I use a loop to generate different column selections based on multiple vectors and output the results in an R console?\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ========== M√âTRIQUES UTILES ==========\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "# BERTScore Accuracy (seuil)\n",
    "def bertscore_accuracy(preds, refs, threshold=0.85):\n",
    "    scores = bertscore.compute(predictions=preds, references=refs, lang=\"en\")[\"f1\"]\n",
    "    binary_preds = [int(s >= threshold) for s in scores]\n",
    "    return sum(binary_preds) / len(scores), binary_preds\n",
    "\n",
    "# ========== CHARGER LES R√âF√âRENCES ==========\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "references = [entry[\"answer\"] for entry in data]\n",
    "\n",
    "# ========== G√âN√âRATION ==========\n",
    "def generate_text(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=300)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# ========== CHARGER LES MOD√àLES ==========\n",
    "models_info = {}\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# --- 0. TinyLlama (base) ---\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(model_name)\n",
    "model_base = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "models_info[\"TinyLlama (base)\"] = (model_base, tokenizer_base)\n",
    "\n",
    "# --- 1. TinyLlama + LoRA ---\n",
    "tokenizer_lora = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model_lora = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "model_lora = PeftModel.from_pretrained(base_model_lora, lora_adapter_path)\n",
    "models_info[\"TinyLlama+LoRA\"] = (model_lora, tokenizer_lora)\n",
    "\n",
    "# ========== √âVALUATION ==========\n",
    "all_results = {}\n",
    "\n",
    "for model_label, (model, tokenizer) in models_info.items():\n",
    "    print(f\"\\nüîÑ G√©n√©ration avec {model_label}\")\n",
    "    generation = generate_text(model, tokenizer, prompt)\n",
    "    print(f\"\\nüß† R√©ponse g√©n√©r√©e :\\n{generation}\\n\")\n",
    "\n",
    "    predictions = [generation] * len(references)\n",
    "\n",
    "    # BERTScore + m√©triques de classification\n",
    "    bert_acc, binary_preds = bertscore_accuracy(predictions, references)\n",
    "    y_true = [1] * len(references)  # Toutes les r√©ponses attendues sont consid√©r√©es comme positives\n",
    "\n",
    "    acc = accuracy_score(y_true, binary_preds)\n",
    "    precision = precision_score(y_true, binary_preds, zero_division=0)\n",
    "    recall = recall_score(y_true, binary_preds, zero_division=0)\n",
    "    f1 = f1_score(y_true, binary_preds, zero_division=0)\n",
    "\n",
    "    all_results[model_label] = {\n",
    "        \"BERTScore Acc\": bert_acc,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1\n",
    "    }\n",
    "\n",
    "# ========== AFFICHAGE FINAL ==========\n",
    "df = pd.DataFrame(all_results).T\n",
    "print(\"\\nüìä R√©sum√© des performances :\\n\")\n",
    "print(df.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9163555-6dc4-4842-9e30-a8c623a3877e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
